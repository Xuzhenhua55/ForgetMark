model_args:
  pretrained_model_name_or_path: "/tools/ModelCheckpoints/meta-llama/Llama-3-8B"
  attn_implementation: 'flash_attention_2'
  torch_dtype: bfloat16
tokenizer_args:
  pretrained_model_name_or_path: "/tools/ModelCheckpoints/meta-llama/Llama-3-8B"
  chat_template: "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"
template_args:
  apply_chat_template: True
  system_prompt: You are a helpful assistant.
  system_prompt_with_special_tokens: "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant.<|eot_id|>"
  user_start_tag: "<|start_header_id|>user<|end_header_id|>\n\n"
  user_end_tag: "<|eot_id|>"
  asst_start_tag: "<|start_header_id|>assistant<|end_header_id|>\n\n"
  asst_end_tag: "<|eot_id|>"
  date_string: 09 Sep 2025
